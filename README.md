<div align="center">

# Omn1-ACE

**Intelligent Context Management for AI Development Tools**

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-brightgreen.svg)](deploy/docker-compose.yml)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)

**[Quick Start](QUICK_START.md)** â€¢ **[Documentation](docs/)** â€¢ **[Architecture](docs/ARCHITECTURE.md)** â€¢ **[Report Issue](https://github.com/mrtozner/omn1-ace/issues)**

---

### ğŸš€ Cut AI API costs by 85%+ with predictive context prefetching

Omn1-ACE uses multi-tier caching, tri-index search, and team learning to deliver only the context you needâ€”saving tokens, time, and money.

</div>

---

## ğŸš§ Project Status

> **Current Stage**: Prototype / Early Development
>
> - âœ… Architecture designed and documented
> - âœ… Infrastructure setup (Docker, databases)
> - âš ï¸ Core API endpoints are placeholders (not yet implemented)
> - âš ï¸ Not production-ready
>
> **For production-ready microservices**, see [OmniMemory](https://github.com/mrtozner/omnimemory)

---

## ğŸ’¡ Why Omn1-ACE?

| Feature | Traditional Approach | Omn1-ACE |
|---------|---------------------|----------|
| **Context Delivery** | Send entire history every query | Send only relevant context (85% reduction) |
| **Token Usage** | 10,000+ tokens per query | ~1,500 tokens per query |
| **Cost** (Claude, 10K queries/month) | ~$450/month | ~$68/month |
| **Team Learning** | Each user rebuilds context | Shared L2 cache learns from team |
| **Search** | Simple keyword matching | Tri-index (semantic + keyword + structural) |
| **Prediction** | Reactive (wait for query) | Proactive (prefetch likely context) |

**Projected Savings**: $382/month per developer at typical usage

---

## âš¡ Key Features

<table>
<tr>
<td width="33%" valign="top">

### ğŸ§  Predictive Prefetching
Multi-strategy prediction engine that anticipates context needs before you ask
- Workflow pattern matching
- Code structure analysis
- Team behavior learning

</td>
<td width="33%" valign="top">

### ğŸ” Tri-Index Search
Three search methods combined for maximum relevance
- **Dense**: Semantic vector search
- **Sparse**: BM25 keyword matching
- **Structural**: AST-based code patterns

</td>
<td width="33%" valign="top">

### ğŸ’¾ Multi-Tier Caching
Three-layer cache architecture optimized for performance and cost
- **L1**: User cache (personal patterns)
- **L2**: Team cache (shared knowledge)
- **L3**: Archive (long-term storage)

</td>
</tr>
</table>

### Additional Capabilities

- **Code-Aware Compression**: 85-94% token reduction while preserving semantic meaning
- **Model-Specific Optimization**: Context tailored for Claude, GPT, or Gemini
- **Team Intelligence**: Cross-user learning and pattern aggregation
- **LSP Integration**: Enhanced code intelligence via Language Server Protocol

---

## ğŸ¯ Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose (recommended)
- 4GB+ RAM

### ğŸ³ Docker Compose (Recommended)

Get started in 5 minutes:

```bash
# Clone the repository
git clone https://github.com/mrtozner/omn1-ace.git
cd omn1-ace

# Copy environment template
cp .env.example .env

# IMPORTANT: Edit .env and change POSTGRES_PASSWORD
nano .env

# Start all services
docker-compose -f deploy/docker-compose.yml up -d

# Verify services
curl http://localhost:8000/health
```

**[ğŸ“– Full Setup Guide â†’](QUICK_START.md)**

---

## ğŸ—ï¸ Architecture

Omn1-ACE implements a 4-layer anticipatory system:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         AI Development Tools                    â”‚
â”‚  (Claude Code, Cursor, Continue, etc.)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Prediction Engine     â”‚  â† Multi-strategy prediction
        â”‚  (Prefetch context)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Tri-Index Search     â”‚  â† Dense + Sparse + Structural
        â”‚  (Find relevant code)  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Multi-Tier Cache     â”‚  â† L1 (user) + L2 (team) + L3
        â”‚  (Smart retrieval)     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Storage Layer        â”‚  â† Qdrant + PostgreSQL + Redis
        â”‚  (Vector DB + Graph)   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**[ğŸ“ Detailed Architecture â†’](docs/ARCHITECTURE.md)**

---

## âš™ï¸ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/api/v1/embeddings` | POST | Generate vector embeddings |
| `/api/v1/search` | POST | Tri-index search (semantic + keyword + structural) |
| `/api/v1/predict` | POST | Get predicted context for current workflow |
| `/api/v1/cache/stats` | GET | Cache performance statistics |
| `/api/v1/compress` | POST | Compress code while preserving semantics |

**Interactive Docs**: `http://localhost:8000/docs` (OpenAPI)

---

## âš ï¸ Multi-Tool Context Considerations

### Context Window Limits

Different AI models have different token limits:

| Model | Context Window | Configuration |
|-------|---------------|---------------|
| **Claude 3.5 Sonnet** | 200,000 tokens | `CLAUDE_CONTEXT_WINDOW=200000` |
| **GPT-4 Turbo** | 128,000 tokens | `GPT_CONTEXT_WINDOW=128000` |
| **Gemini 1.5 Pro** | 1,000,000 tokens | `GEMINI_CONTEXT_WINDOW=1000000` |
| **GPT-3.5 Turbo** | 16,000 tokens | `GPT_CONTEXT_WINDOW=16000` |

**Impact**: Context optimized for Gemini may exceed GPT-4's limits.

### Configuration

Set your target model in `.env`:

```bash
DEFAULT_TARGET_MODEL=claude  # or gpt, gemini
CLAUDE_CONTEXT_WINDOW=200000
GPT_CONTEXT_WINDOW=128000
GEMINI_CONTEXT_WINDOW=1000000
```

### Model-Specific Behavior

**Claude (Anthropic)**:
- âœ… Best with structured, detailed context
- âœ… Excellent at following complex instructions
- âš¡ Prefers explicit task breakdowns

**GPT (OpenAI)**:
- âœ… Works well with conversational context
- âš ï¸ May need more explicit formatting
- âš¡ Better with shorter, focused context

**Gemini (Google)**:
- âœ… Handles very large context windows
- âœ… Good with multimodal content
- âš ï¸ May need different prompt engineering

**Recommendation**: Standardize on one model per team for consistent experience.

---

## ğŸ“Š Performance

### Recommended Resources

| Component | Requirements |
|-----------|-------------|
| **API Server** | 2+ CPU cores, 4GB+ RAM |
| **PostgreSQL** | 4GB+ RAM, SSD storage |
| **Qdrant** | 8GB+ RAM (scales with corpus) |
| **Redis** | 2GB+ RAM (scales with cache) |

### Scaling

- **Horizontal**: API servers behind load balancer
- **PostgreSQL**: Read replicas for read-heavy workloads
- **Qdrant**: Clustering for large-scale vector search
- **Redis**: Clustering for high-availability caching

---

## ğŸ”’ Security

**Before production deployment**:

1. âœ… Change all default passwords in `docker-compose.yml`
2. âœ… Use environment variables for sensitive configuration
3. âœ… Enable TLS/SSL for all service connections
4. âœ… Configure authentication for API endpoints
5. âœ… Use network policies to restrict service access
6. âœ… Regular security updates for all dependencies

---

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**Before submitting a PR**:
- All tests pass
- Code follows style guidelines (black, isort, pylint)
- New features include tests
- Documentation is updated

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) for details.

---

## ğŸ”— Related Projects

- **[OmniMemory](https://github.com/mrtozner/omnimemory)**: Production-ready microservices (13 independent services)
- **Extensions**: LSP integration for enhanced code intelligence ([docs](extensions/lsp/README.md))

---

## ğŸ™ Acknowledgments

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - Modern web framework
- [Qdrant](https://qdrant.tech/) - Vector similarity search
- [PostgreSQL](https://www.postgresql.org/) - Relational database
- [Redis](https://redis.io/) - In-memory data store
- [NetworkX](https://networkx.org/) - Graph analysis

---

<div align="center">

**[â­ Star this repo](https://github.com/mrtozner/omn1-ace)** if you find it useful!

**[ğŸ“– Read the Docs](docs/)** â€¢ **[ğŸ’¬ Discussions](https://github.com/mrtozner/omn1-ace/discussions)** â€¢ **[ğŸ› Report Bug](https://github.com/mrtozner/omn1-ace/issues)**

Made with â¤ï¸ by [Mert Ozoner](https://github.com/mrtozner)

</div>
